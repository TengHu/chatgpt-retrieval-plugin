{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document\n",
    "### see https://github.com/pinecone-io/examples/blob/master/generation/chatgpt/plugins/langchain-docs-plugin.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU langchain tiktoken tqdm\n",
    "!pip install beautifulsoup4\n",
    "!pip install -qU python-dotenv\n",
    "# eval \"$(direnv hook bash)\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: wget: command not found\n"
     ]
    }
   ],
   "source": [
    "!wget -r -A.html -P rtdocs https://python.langchain.com/en/latest/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tenghu/Code/chatgpt_plugins/chatgpt-retrieval-plugin/env/lib/python3.8/site-packages/langchain/document_loaders/readthedocs.py:30: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 30 of the file /Users/tenghu/Code/chatgpt_plugins/chatgpt-retrieval-plugin/env/lib/python3.8/site-packages/langchain/document_loaders/readthedocs.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  _ = BeautifulSoup(\n",
      "/Users/tenghu/Code/chatgpt_plugins/chatgpt-retrieval-plugin/env/lib/python3.8/site-packages/langchain/document_loaders/readthedocs.py:46: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 46 of the file /Users/tenghu/Code/chatgpt_plugins/chatgpt-retrieval-plugin/env/lib/python3.8/site-packages/langchain/document_loaders/readthedocs.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(data, **self.bs_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "472"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import ReadTheDocsLoader\n",
    "\n",
    "loader = ReadTheDocsLoader('rtdocs')\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=20,  # number of tokens overlap between chunks\n",
    "    length_function=tiktoken_len,\n",
    "    separators=['\\n\\n', '\\n', ' ', '']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.text_splitter.RecursiveCharacterTextSplitter at 0x11543e700>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = text_splitter.split_text(docs[5].page_content)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ask Questions On Your Custom (or Private) Files\\nConnect Google Drive Files To OpenAI\\nYouTube Transcripts + OpenAI\\nQuestion A 300 Page Book (w/ OpenAI + Pinecone)\\nWorkaround OpenAI's Token Limit With Chain Types\\nBuild Your Own OpenAI + LangChain Web App in 23 Minutes\\nWorking With The New ChatGPT API\\nOpenAI + LangChain Wrote Me 100 Custom Sales Emails\\nStructured Output From OpenAI (Clean Dirty Data)\\nConnect OpenAI To +5,000 Tools (LangChain + Zapier)\\nUse LLMs To Extract Data From Text (Expert Mode)\\nLangChain How to and guides by Sam Witteveen:\\nLangChain Basics - LLMs & PromptTemplates with Colab\\nLangChain Basics - Tools and Chains\\nChatGPT API Announcement & Code Walkthrough with LangChain\\nConversations with Memory (explanation & code walkthrough)\\nChat with Flan20B\\nUsing Hugging Face Models locally (code walkthrough)\\nPAL : Program-aided Language Models with LangChain code\\nBuilding a Summarization System with LangChain and GPT-3 - Part 1\\nBuilding a Summarization System with LangChain and GPT-3 - Part 2\\nMicrosoft’s Visual ChatGPT using LangChain\\nLangChain Agents - Joining Tools and Chains with Decisions\\nComparing LLMs with LangChain\\nUsing Constitutional AI in LangChain\\nTalking to Alpaca with LangChain - Creating an Alpaca Chatbot\\nTalk to your CSV & Excel with LangChain\\nBabyAGI: Discover the Power of Task-Driven Autonomous Agents!\\nImprove your BabyAGI with LangChain\\nLangChain by Prompt Engineering:\\nLangChain Crash Course — All You Need to Know to Build Powerful Apps with LLMs\\nWorking with MULTIPLE PDF Files in LangChain: ChatGPT for your Data\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://python.langchain.com/en/latest/youtube.html\n",
      "001b1930c81f\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "m = hashlib.md5()  # this will convert URL into unique ID\n",
    "\n",
    "url = docs[5].metadata['source'].replace('rtdocs/', 'https://')\n",
    "print(url)\n",
    "\n",
    "# convert URL to unique ID\n",
    "m.update(url.encode('utf-8'))\n",
    "uid = m.hexdigest()[:12]\n",
    "print(uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m data \u001b[39m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     {\n\u001b[1;32m      3\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00muid\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m: chunk,\n\u001b[1;32m      5\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m'\u001b[39m: {\u001b[39m'\u001b[39m\u001b[39murl\u001b[39m\u001b[39m'\u001b[39m: url}\n\u001b[0;32m----> 6\u001b[0m     } \u001b[39mfor\u001b[39;00m i, chunk \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(chunks)\n\u001b[1;32m      7\u001b[0m ]\n\u001b[1;32m      8\u001b[0m data\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chunks' is not defined"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    {\n",
    "        'id': f'{uid}-{i}',\n",
    "        'text': chunk,\n",
    "        'metadata': {'url': url}\n",
    "    } for i, chunk in enumerate(chunks)\n",
    "]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 472/472 [00:02<00:00, 212.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1945"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "documents = []\n",
    "\n",
    "for doc in tqdm(docs):\n",
    "    url = doc.metadata['source'].replace('rtdocs/', 'https://')\n",
    "    m.update(url.encode('utf-8'))\n",
    "    uid = m.hexdigest()[:12]\n",
    "    chunks = text_splitter.split_text(doc.page_content)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        documents.append({\n",
    "            'id': f'{uid}-{i}',\n",
    "            'text': chunk,\n",
    "            'metadata': {'url': url}\n",
    "        })\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJjaGF0Z3B0IHBsdWdpbiBhcHAiLCJuYW1lIjoibmllbCIsImlhdCI6MTY4MzQxMzM1OX0.FRvcY0cASN_6JKXWOItD_GKRhWphHNDQs_49srGLz8E'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "BEARER_TOKEN = os.environ.get(\"BEARER_TOKEN\") or \"BEARER_TOKEN_HERE\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {BEARER_TOKEN}\"\n",
    "}\n",
    "BEARER_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:31<00:00,  4.60s/it]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "from tqdm.auto import tqdm\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "endpoint_url = \"https://squid-app-ercsl.ondigitalocean.app\"\n",
    "s = requests.Session()\n",
    "\n",
    "# we setup a retry strategy to retry on 5xx errors\n",
    "retries = Retry(\n",
    "    total=5,  # number of retries before raising error\n",
    "    backoff_factor=0.1,\n",
    "    status_forcelist=[500, 502, 503, 504]\n",
    ")\n",
    "\n",
    "# s.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "\n",
    "for i in tqdm(range(0, len(documents), batch_size)):\n",
    "    i_end = min(len(documents), i+batch_size)\n",
    "    # make post request that allows up to 5 retries\n",
    "    res = s.post(\n",
    "        f\"{endpoint_url}/upsert\",\n",
    "        headers=headers,\n",
    "        json={\n",
    "            \"documents\": documents[i:i_end]\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = [\n",
    "    {'query': \"What is the LLMChain in LangChain?\"},\n",
    "    {'query': \"How do I use Pinecone in LangChain?\"},\n",
    "    {'query': \"What is the difference between Knowledge Graph memory and buffer memory for \"+\n",
    "     \"conversational memory?\"}\n",
    "]\n",
    "\n",
    "res = requests.post(\n",
    "    f\"{endpoint_url}/query\",\n",
    "    headers=headers,\n",
    "    json={\n",
    "        'queries': queries\n",
    "    }\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "What is the LLMChain in LangChain?\n",
      "\n",
      "0.87: Loading from LangChainHub next Sequential Chains  Contents    LLM Chain Additional ways of running LLM Chain Parsing the outputs Initialize from string By Harrison Chase            © Copyright 2023, Harrison Chase.          Last updated on May 07, 2023.\n",
      "0.87: nThese are, in increasing order of complexity:\\n\\nð\\x9f“\\x83 LLMs and Prompts:\\n\\nThis includes prompt management, prompt optimization, a generic interface for all LLMs, and common utilities for working with LLMs.\\n\\nð\\x9f”\\x97 Chains:\\n\\nChains go beyond a single LLM call and involve sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and\n",
      "0.87: Started\\nModules\\nUse Cases\\nReference Docs\\nLangChain Ecosystem\\nAdditional Resources\\n\\n\\n\\n\\n\\n\\n\\n\\nWelcome to LangChain#\\nLarge language models (LLMs) are emerging as a transformative technology, enabling\\ndevelopers to build applications that they previously could not.\\nBut using these LLMs in isolation is often not enough to\\ncreate a truly powerful app - the real power comes when you are able to\\ncombine them with other sources of computation or knowledge.\\nThis library is aimed at assisting in the development of those types of applications.\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "How do I use Pinecone in LangChain?\n",
      "\n",
      "0.85: .md .pdf Pinecone  Contents  Installation and Setup Wrappers VectorStore Pinecone# This page covers how to use the Pinecone ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific Pinecone wrappers. Installation and Setup# Install the Python SDK with pip install pinecone-client Wrappers# VectorStore# There exists a wrapper around Pinecone indexes, allowing you to use it as a vectorstore, whether for semantic search or example selection. To import this vectorstore: from langchain.vectorstores import Pinecone For a more detailed walkthrough of the Pinecone wrapper, see this notebook previous PGVector next PipelineAI  Contents    Installation and Setup Wrappers VectorStore By Harrison Chase            © Copyright 2023, Harrison Chase.          Last updated on May 07, 2023.\n",
      "0.82: Creating a Pinecone index\n",
      "0.82: NOTE: The self-query retriever requires you to have lark installed (pip install lark) # !pip install lark import os import pinecone pinecone.init(api_key=os.environ[\"PINECONE_API_KEY\"], environment=os.environ[\"PINECONE_ENV\"]) /Users/harrisonchase/.pyenv/versions/3.9.1/envs/langchain/lib/python3.9/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)   from tqdm.autonotebook import tqdm from langchain.schema import Document from langchain.embeddings.openai import OpenAIEmbeddings from langchain.vectorstores import Pinecone embeddings = OpenAIEmbeddings()\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "What is the difference between Knowledge Graph memory and buffer memory for conversational memory?\n",
      "\n",
      "0.84: .ipynb .pdf Conversation Knowledge Graph Memory  Contents  Using in a chain Conversation Knowledge Graph Memory# This type of memory uses a knowledge graph to recreate memory. Let’s first walk through how to use the utilities from langchain.memory import ConversationKGMemory from langchain.llms import OpenAI llm = OpenAI(temperature=0) memory = ConversationKGMemory(llm=llm) memory.save_context({\"input\": \"say hi to sam\"}, {\"ouput\": \"who is sam\"}) memory.save_context({\"input\": \"sam is a friend\"}, {\"ouput\": \"okay\"}) memory.load_memory_variables({\"input\": 'who is sam'}) {'history': 'On Sam: Sam is friend.'} We can also get the history as a list of messages (this is useful if you are using this with a chat model). memory = ConversationKGMemory(llm=llm, return_messages=True) memory.\n",
      "0.83: .md .pdf Chatbots Chatbots# Conceptual Guide Since language models are good at producing text, that makes them ideal for creating chatbots. Aside from the base prompts/LLMs, an important concept to know for Chatbots is memory. Most chat based applications rely on remembering what happened in previous interactions, which memory is designed to help with. The following resources exist: ChatGPT Clone: A notebook walking through how to recreate a ChatGPT-like experience with LangChain. Conversation Memory: A notebook walking through how to use different types of conversational memory. Conversation Agent: A notebook walking through how to create an agent optimized for conversation. Additional related resources include: Memory Key Concepts: Explanation of key concepts related to memory. Memory Examples: A collection of how-to examples for working with memory. More end-to-end examples include: Voice Assistant: A notebook walking through how to create a voice assistant using LangChain. previous Question Answering over Docs next\n",
      "0.83: .ipynb .pdf Getting Started  Contents  ChatMessageHistory ConversationBufferMemory Using in a chain Saving Message History Getting Started# This notebook walks through how LangChain thinks about memory. Memory involves keeping a concept of state around throughout a user’s interactions with an language model. A user’s interactions with a language model are captured in the concept of ChatMessages, so this boils down to ingesting, capturing, transforming and extracting knowledge from a sequence of chat messages. There are many different ways to do this, each of which exists as its own memory type. In general, for each type of memory there are two ways to understanding using memory. These are the standalone functions which extract information from a sequence of messages, and then there is the way you can use this type of memory in a chain. Memory can return multiple pieces of information (for example, the most recent N messages and a summary of all previous messages).\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query_result in res.json()['results']:\n",
    "    query = query_result['query']\n",
    "    answers = []\n",
    "    scores = []\n",
    "    for result in query_result['results']:\n",
    "        answers.append(result['text'])\n",
    "        scores.append(round(result['score'], 2))\n",
    "    print(\"-\"*70+\"\\n\"+query+\"\\n\\n\"+\"\\n\".join([f\"{s}: {a}\" for a, s in zip(answers, scores)])+\"\\n\"+\"-\"*70+\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
